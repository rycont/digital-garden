---
title: DaramGPT 성능 평가
date: 2023-06-07T16:12:38.400Z
---

# DaramGPT 성능 평가

대조군은 Teacher 모델인 [[KoGPT Trinity]]와 가장 널리 사용되고 있는 한국어 생성모델인 [[KoGPT2]]로 설정하였습니다.

|            | DarmaGPT( Ours)  | KoGPT2             | KoGPT Trinity                |
| ---------- | ---------------- | ------------------ | ---------------------------- |
| Params     | 67M (67,656,960) | 125M (125,164,800) | 1160M (1,160,315,520)        |
| Model Size | 429MB            | 513MB              | 4,675MB                      |
| Embeddings | 768              | 768                | 1920                         |
| Heads      | 8                | 12                 | 16                           |
| Layers     | 4                | 12                 | 24                           |
| Model Card |                  | skt/kogpt2-base-v2 | skt/ko-gpt-trinity-1.2B-v0.5 |

학습이 끝난 모델의 성능을 측정하기 위해 몇몇 Downstream Task를 학습하였습니다. [[DaramGPT]]와 KoGPT2, KoGPT Trinity 세가지 모델을 모두 테스트하고자 하였지만, KoGPT Trinity는 Downstream task를 미세조정할 수 있는 컴퓨팅 성능이 마련되지 않아 기존에 공개되어있던 오피셜 평가지표를 인용하였습니다.

모델 성능은 [[KoBEST]], [[NSMC]]로 측정하였습니다. 한 Epoch이 끝날 때 마다 Test set의 점수를 측정하여 가장 높았던 점수를 최종 점수로 선정하였습니다. 하이퍼파라미터는 다음과 같습니다

- Epoch: 20회

- Learning Rate: KoBEST 논문에 따름 (KoBEST 데이터가 아닌 NSMC 제외)

- Batch Size: 64

- Sequence Length: 512

- WiC: 1e-6

- BoolQ: 5e-6

## **평가 결과**

퍼센트로 표기된 성능지표는 KoGPT2 대비 DaramGPT의 성능을 계산하였습니다

### **Downstream Tasks**

(소수점 두번째 자리 수 아래는 버림)

|                 | WiC (F1)       | CoPA (F1)      | BoolQ (F1)     | NSMC (Acc)     |
| --------------- | -------------- | -------------- | -------------- | -------------- |
| DaramGPT (Ours) | 65.60 (99.37%) | 66.82 (98.77%) | 66.89 (96.87%) | 86.50 (98.34%) |
| KoGPT2          | 66.01          | 67.65          | 69.05          | 87.96          |
| KoGPT Trinity   | 68.4           | 79.3           | 73.5           | NA             |

### **학습 소요 시간**

(소수점 두번째 자리 수 아래는 버림)

| (초)            | WiC          | CoPA         | BoolQ         | NSMC          |
| --------------- | ------------ | ------------ | ------------- | ------------- |
| DaramGPT (Ours) | 15 (50.5% ↓) | 16 (58.9% ↓) | 45 (60.17% ↓) | 777 (62.6% ↓) |
| KoGPT2          | 38           | 39           | 113           | 2081          |

또한, 모델의 실행 및 학습 시간에 괄목할만한 개선이 있었습니다. KoGPT2 대비 학습에 소요되는 시간이 60%가량 감소하였습니다. 파라미터 수 감소에서 직접적으로 나타나는 효과인 실행 시간 단축이 드러났으니, 필요한 컴퓨팅 자원의 규모 감소, 더 적은 전기 사용 등의 다른 효과도 나타났을것이라 예상할 수 있습니다.

## **결론**

한국어 대규모 언어모델을 증류하여, 일반성능 디바이스에서도 실행할 수 있는 크기의 모델을 만들었습니다. 기존에 공개되었던 엔트리급 모델에 비해 성능은 거의 유지하면서 학습 소요시간을 획기적으로 줄일 수 있었습니다. 본 연구를 통해 더 많은 기기와 사용자에게 생성형 언어모델을 제공할 수 있는 초석을 마련하였습니다.
